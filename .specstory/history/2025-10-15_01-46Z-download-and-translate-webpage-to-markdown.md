<!-- Generated by SpecStory -->

<!-- cursor Session ad933bd6-16a3-4deb-86e9-1262359aac22 (2025-10-15 01:46Z) -->

# Download and translate webpage to markdown (2025-10-15 01:46Z)

_**User (2025-10-14 10:38Z)**_

@https://www.motivenotes.ai/p/what-makes-5-of-ai-agents-actually 下载当前网页，翻译成中文，保存markdown格式。

---

_**Assistant (claude-4.5-sonnet-thinking)**_

<think><details><summary>Thought Process</summary>用户想要我下载这个网页，翻译成中文，并保存为markdown格式。我已经在websearch结果中获取到了这个网页的内容。

现在我需要：
1. 将这个网页内容翻译成中文
2. 保存为markdown格式

让我先翻译这篇文章，然后保存为markdown文件。</details></think>

---

我将帮您将这个网页翻译成中文并保存为markdown格式。

---

Tool use: **write**


<details><summary>Edit file: 是什么让5%的AI代理在生产环境中真正起作用.md</summary>

**Chunk 1**
Lines added: 274, lines removed: 1

```diff
@@ -1,1 +1,274 @@
- 
+ # 是什么让5%的AI代理在生产环境中真正起作用？
+ 
+ ### 超越提示词：来自上下文前沿的笔记
+ 
+ Oana Olteanu
+ 
+ 2025年10月2日
+ 
+ ---
+ 
+ 大多数创始人认为他们在构建AI产品。但实际上他们在构建的是**上下文选择系统**。
+ 
+ 这个周一，我在旧金山主持了一场小组讨论，参与者包括来自Uber、WisdomAI、EvenUp和Datastrato的工程师和机器学习负责人。这场名为"超越提示词"的活动吸引了600多名注册者，主要是创始人、工程师和早期AI产品构建者。
+ 
+ 我们不是在那里重复提示工程技巧。
+ 
+ 我们讨论的是上下文工程、推理栈设计，以及在企业环境中扩展代理系统所需的条件。如果说"提示"是冰山一角，那么这个小组讨论深入到了下面冷酷而复杂的主体：上下文选择、语义层、内存编排、治理和多模型路由。
+ 
+ **现实检验来了**：一位小组成员提到，95%的AI代理部署在生产环境中失败。不是因为模型不够智能，而是因为围绕它们的脚手架——上下文工程、安全性、内存设计——还不够完善。
+ 
+ 当晚一个让我印象深刻的比喻：
+ 
+ > "基础模型是土壤；上下文是种子。"
+ 
+ 我一直对语义层着迷，不是因为它们很炫，而是因为它们是创始人悄悄地将信任、实用性和差异化构建到LLM系统中的地方。我见过太多团队将提示与产品混为一谈。这个小组讨论感觉像是一个时刻，真正的工程工作开始得到应有的重视。
+ 
+ 以下是要点，不仅仅是引用，而是我在认真的AI团队中看到的重复模式。如果你在基础设施、工具或垂直AI层面构建，这是你需要做对的脚手架。
+ 
+ ## **上下文工程 ≠ 提示黑客**
+ 
+ 几位小组成员都表达了同样的见解：**微调很少是必要的**。检索增强生成(RAG)如果做得好，就足够了。但今天大多数RAG系统都太天真了。
+ 
+ **失败模式：**
+ 
+ * 索引所有内容 → 检索太多 → 混淆模型
+ * 索引太少 → 让模型缺乏信号
+ * 混合结构化+非结构化数据 → 破坏嵌入或扁平化关键模式
+ 
+ 那么，高级上下文工程实际上是什么样的？
+ 
+ ### **a) LLM的特征选择**
+ 
+ 一位演讲者将上下文工程重新定义为LLM原生特征工程：
+ 
+ * **选择性上下文修剪** = 特征选择
+ * **上下文验证** = 模式/类型/时效性检查
+ * **"上下文可观察性"** = 跟踪哪些输入改善/恶化了输出质量
+ * **带元数据的嵌入增强** = 类型化特征 + 条件
+ 
+ 这个框架很重要。它意味着你可以将上下文视为一个版本化的、可审计的、可测试的工件，而不是一个字符串块。
+ 
+ ### **b) 语义 + 元数据分层**
+ 
+ 几个团队描述了双层架构：
+ 
+ * **语义层** → 经典向量搜索
+ * **元数据层** → 根据文档类型、时间戳、访问权限或垂直本体强制执行过滤器
+ 
+ 这个混合层有助于规范混乱的输入格式(PDF、音频、日志、指标)，并确保你不仅仅是检索"相似内容"，而是**相关的结构化知识**。想想：分类法、实体链接和嵌入之上的特定领域模式。
+ 
+ ### **c) Text-to-SQL的现实检验**
+ 
+ 当主持人问观众"你们中有多少人构建了text-to-SQL并将其投入生产？"时，没有一只手举起。
+ 
+ 这不是因为问题小众，而是因为**查询理解极其困难**。自然语言是模糊的。业务术语是特定领域的。而且LLM不知道你公司对"收入"或"活跃用户"的定义，除非有广泛的上下文工程。
+ 
+ 成功的团队不只是向模型抛出SQL模式。他们构建：
+ 
+ * 业务术语表和术语映射
+ * 带约束的查询模板
+ * 在执行前捕获语义错误的验证层
+ * 随时间改进理解的反馈循环
+ 
+ ## **治理与信任不是"仅限企业"的问题**
+ 
+ 安全性、血统和权限管理一再出现，不是作为勾选框项目，而是作为**部署的阻碍因素**。
+ 
+ ### **在垂直领域构建的创始人：请注意**
+ 
+ * **你必须追踪哪些输入导致哪些输出**(血统)
+ * **你必须尊重行级别、基于角色的访问**(策略门控)
+ * **你必须允许用户特定的输出，即使提示相同**
+ 
+ 一位演讲者说：
+ 
+ > "如果两名员工问同样的问题，模型输出应该不同，因为他们有不同的权限。"
+ 
+ 没有这些控制，你的代理可能在功能上是正确的，但在组织上是错误的——泄露访问权限或违反合规性。
+ 
+ 这里的领先模式：**统一的元数据目录**，用于结构化+非结构化数据，在索引+查询时嵌入访问策略。
+ 
+ ### **信任问题是人的问题，而非技术问题**
+ 
+ 一位小组成员分享了一个个人故事，阐明了这一挑战：他的妻子拒绝让他使用特斯拉的自动驾驶。为什么？不是因为它不工作，而是因为她不信任它。
+ 
+ > "当AI触及你的安全、你的金钱等非常敏感的部分时，你信任AI吗？我认为这是一个大的阻碍因素。我们有时做AI代理，但人类在思考：我真的信任AI吗？"
+ 
+ 这不仅仅是消费产品的问题。同样的障碍存在于做出关于收入确认、医疗记录或合规报告决策的企业AI代理。信任不是关于原始能力，而是关于一致、可解释、可审计的行为。
+ 
+ 成功的5%的AI代理？它们都有一个共同点：**人在环中的设计**。它们将AI定位为助手，而不是自主决策者。它们创建反馈循环，系统从纠正中学习。它们使人类易于验证和覆盖。
+ 
+ ## **内存不仅仅是存储，它是架构**
+ 
+ 每个人都想"添加内存"。但内存不是一个功能，它是一个设计决策，涉及UX、隐私和系统影响。
+ 
+ ### **内存级别：**
+ 
+ * **用户级别**：偏好(例如，图表类型、风格、写作语气)
+ * **团队级别**：重复查询、仪表板、运行手册
+ * **组织级别**：机构知识、政策、先前的决策
+ 
+ 大多数初创公司将内存硬编码到应用逻辑或本地存储中。但最好的团队将其抽象为**上下文层+行为层**，版本化和可组合。一位演讲者将此描述为：
+ 
+ > "语义内存 + 分类法 + 运行手册 = 上下文。  
+ > 个人偏好 = 内存。"
+ 
+ ### **内存作为个性化**
+ 
+ 在应用层面，内存有两个目的：
+ 
+ 1. **定制行为**以适应个人用户——他们的写作风格、首选格式、领域专业知识
+ 2. **主动协助**基于事件和元数据，而不仅仅是被动的聊天响应
+ 
+ 一个团队描述了在Uber构建对话式BI工具。冷启动问题？用户不知道要问什么。解决方案？从他们过去的查询日志中构建内存，然后建议相关问题作为对话启动器，就像一个好的主人记得你上次谈论的内容。
+ 
+ 但这里有紧张关系：有用的个性化何时会跨越到令人毛骨悚然的监视？
+ 
+ 一位小组成员描述了向ChatGPT询问家庭电影推荐，却发现它用他孩子的名字Claire和Brandon提供了量身定制的建议。他的反应？"我不喜欢这个答案。你为什么这么了解我的儿子和女儿？不要触碰我的隐私。"
+ 
+ ### **设计张力：**
+ 
+ * 内存改善UX和代理流畅性
+ * 但过度个性化可能很快进入隐私领域
+ * 而且共享内存可能会破坏访问控制，除非仔细划定范围
+ 
+ **这里缺少一个原语**：一个安全的、可移植的内存层，可以跨应用工作，可供用户使用，而不是锁定在提供商内部。还没有人完美地实现它。一位小组成员说，如果他不在构建他目前的创业公司，这将是他的下一个项目。
+ 
+ 如果你正在构建这个，请联系我。
+ 
+ ## **多模型推理和编排模式**
+ 
+ 另一个新兴设计：**模型编排**。
+ 
+ 在生产环境中，你不只是为所有事情调用GPT-4。团队越来越多地基于以下内容运行模型路由逻辑：
+ 
+ * 任务复杂性
+ * 延迟约束
+ * 成本敏感性
+ * 数据本地性/监管问题
+ * 查询类型(例如，摘要 vs 语义搜索 vs 结构化问答)
+ 
+ ### **示例模式：**
+ 
+ * 对于琐碎的查询 → 本地模型(无网络调用)
+ * 对于结构化查询 → 调用DSL → SQL转换器
+ * 对于复杂分析 → 调用OpenAI / Anthropic / Gemini
+ * 回退或验证 → 双模型冗余(评判者+响应者)
+ 
+ 这更接近**编译器设计而不是webapp路由**。你不仅仅是"发送到LLM"，你正在跨异构模型、工具和验证运行决策的DAG。
+ 
+ ### **为什么重要：**
+ 
+ 如果你的系统随着使用增长而变得更慢或更昂贵，这是首先要重新审视的层。如果你希望AI对用户来说感觉无缝，路由不能永远脆弱或手动调整。你需要自适应策略。
+ 
+ 一个团队描述了他们的方法：简单的问题去小型、快速的模型。复杂的推理任务被路由到前沿模型。关键见解？模型选择本身可以通过跟踪哪些查询在哪些模型上成功来随时间学习。
+ 
+ ## **聊天何时真的是正确的界面？**
+ 
+ 不是每个任务都需要聊天机器人。
+ 
+ 一位观众成员直接挑战了这个前提："我不确定自然语言总是比GUI更可取。如果我在订Uber，我不想对着手机说话。我只是点击、点击、点击，我就有了我的车。"
+ 
+ 小组的共识：**当对话消除学习曲线时，它就有效**。
+ 
+ 对于像BI仪表板或数据分析这样传统上需要专业知识的复杂工具，自然语言降低了进入门槛。但一旦你有了答案，用户通常想要GUI控件——将饼图切换到条形图不应该需要更多的打字。
+ 
+ ### **混合模式：**
+ 
+ * 从聊天开始，实现零学习曲线的入口
+ * 提供GUI控件用于细化和迭代
+ * 让用户根据任务和偏好选择他们的模式
+ 
+ 一位小组成员描述了两个NLP的完美用例：
+ 
+ 1. **偶发的、情绪化的任务**，如客户服务，有人感到沮丧，只想发泄或寻求帮助，而无需浏览菜单
+ 2. **探索性、开放式查询**，如"帮我找一个靠近加州、第一排、有海景和蓝天的Airbnb"，其中要求复杂且与上下文相关
+ 
+ 关键见解：我们应该理解人们**为什么**想使用自然语言，并为该意图设计，而不是强制每次交互都进入聊天。
+ 
+ ## **仍然缺失的内容(以及构建者可以获胜的地方)**
+ 
+ 出现了几个感觉未被充分探索的想法——真正的原语等待被产品化：
+ 
+ ### **上下文可观察性**
+ 
+ 什么输入持续改善输出？什么类型的上下文导致幻觉？你如何像测试模型提示一样测试上下文？
+ 
+ 现在，大多数团队都在盲飞——他们没有系统的方法来衡量哪些上下文实际上有帮助而不是有害于模型性能。
+ 
+ ### **可组合内存**
+ 
+ 内存能否与用户一起存在(而不是应用)，可移植且安全，具有组织vs团队vs个人状态的选择加入层？
+ 
+ 这解决了两个问题：
+ 
+ 1. 用户不必在每个新工具中重建他们的上下文
+ 2. 隐私和安全由用户控制，而不是供应商锁定
+ 
+ 这是栈中最大的缺失原语。
+ 
+ ### **领域感知DSL**
+ 
+ 业务用户想要的大多数东西是结构化和重复的。我们为什么还在尝试将自然语言解析为脆弱的SQL，而不是定义更高级别的、约束安全的DSL？
+ 
+ 一个团队建议，与其text-to-SQL，我们应该构建语义业务逻辑层——"显示第四季度收入"应该映射到已验证的计算，而不是原始SQL生成。
+ 
+ ### **延迟感知UX**
+ 
+ 一位小组成员描述了一个内存增强的聊天机器人，它响应缓慢，但令人愉快。为什么？它根据用户上周的提问显示了一系列智能后续问题。
+ 
+ 这里有一个UX解锁点：**异步、主动的AI**，而不仅仅是聊天。想想：在会议前准备简报的代理，当你打开文档时显示相关上下文，或者在你询问之前提醒你数据中的异常。
+ 
+ 关键见解：不同的任务有不同的延迟要求。笑话应该是即时的。如果显示进度并感觉智能，深度分析可以需要10秒。
+ 
+ ## **我将关注什么**
+ 
+ 我离开这个小组讨论时更加确信，我们即将看到一波基础工具——内存工具包、编排层、上下文可观察性——在事后看来会显而易见。但它们今天是混乱和未解决的。
+ 
+ GenAI中的下一个真正护城河不会来自模型访问，它们将来自：
+ 
+ * **上下文质量**
+ * **内存设计**
+ * **编排可靠性**
+ * **信任UX**
+ 
+ 如果你是一个构建基础设施、应用或代理的创始人：你的路线图中有多少明确地解决了这四个问题？
+ 
+ ## **创始人：5个难题要问自己**
+ 
+ 作为上下文/代理系统构建者，试试这些：
+ 
+ **1. 我的应用的上下文预算是多少？** 
+  (理想的上下文窗口大小是多少，我如何优化进入其中的内容？)
+ 
+ **2. 我的内存边界是什么？** 
+  (什么存在于用户级别 vs 团队 vs 组织？它存储在哪里，用户能看到它吗？)
+ 
+ **3. 我能追踪输出血统吗？** 
+  (我能调试LLM响应并知道哪个输入导致了它吗？)
+ 
+ **4. 我使用一个模型还是多个？** 
+  (我如何按复杂性、延迟或成本路由请求？)
+ 
+ **5. 我的用户会信任这个系统处理金钱或医疗数据吗？** 
+  (如果不会，我的安全性或反馈循环缺少什么？)
+ 
+ 如果你在这个层面构建，我想听听你的声音。特别是如果你在其他人称之为基础设施之前就在做这件事。
+ 
+ 如果你是技术读者，特别是在基础设施或AI/ML方面，让我知道：你想要更深入的系列，关于上下文修剪模式、构建双层上下文系统、内存抽象或设计治理吗？
+ 
+ 或者只是回复你现在最大的"上下文工程"头痛问题，我很乐意深入研究。
+ 
+ ---
+ 
+ *更多文章：语义层、微调、创始人心智软件、规范化创始人之旅、AI工程师世界博览会2025 - 现场笔记*
+ 
+ ---
+ 
+ **原文链接：** https://www.motivenotes.ai/p/what-makes-5-of-ai-agents-actually
+ 
+ **作者：** Oana Olteanu
+ 
+ **发布日期：** 2025年10月2日
+ 
```

</details>

---

<think><details><summary>Thought Process</summary>我已经成功将网页内容翻译成中文并保存为markdown格式。文件名是"是什么让5%的AI代理在生产环境中真正起作用.md"。

现在我应该告诉用户已经完成了任务。</details></think>

---

已完成！我已经将这篇关于AI代理的文章翻译成中文并保存为markdown格式。

**保存路径：** `是什么让5%的AI代理在生产环境中真正起作用.md`

这篇文章的主要内容包括：

1. **上下文工程的重要性** - 不仅仅是提示词优化，而是系统的上下文选择和管理
2. **95%的AI代理失败原因** - 缺乏完善的上下文工程、安全性和内存设计
3. **关键技术要点**：
   - 特征选择和上下文修剪
   - 语义层和元数据分层
   - Text-to-SQL的挑战
   - 治理与信任机制
   - 内存架构设计
   - 多模型编排
   - 聊天界面的适用场景

4. **未来方向**：上下文可观察性、可组合内存、领域感知DSL、延迟感知UX

文章强调了成功的AI代理系统需要：上下文质量、内存设计、编排可靠性和信任UX这四个核心要素。

---

